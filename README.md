# Awesome-VQA-LLM [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

A collection of AWESOME things about **Vision Question Answering (VQA) Related Large Language Models (LLMs)**.

Large Language Models demonstrates promising ability of general knowledge and human-like thinking. There have been tons of research and applications in the field of Natural Language Processing but its magic power remains under explored in many other fields in today's AI academic world. This repo aims to provide a curated list of papers, code repos, datasets and resources focusing on the topic of utilizing LLMs in the task of VQA.

## Table of Contents

- [Awesome-VQA-LLM](#awesome-vqa-llm)
  - [Table of Contents](#table-of-contents)
  - [Papers](#papers)
    - [2023](#2023)
  - [Contributing](#contributing)

## Papers
### 2023
- (*CVPR 2023*) From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models [[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Guo_From_Images_to_Textual_Prompts_Zero-Shot_Visual_Question_Answering_With_CVPR_2023_paper.pdf)][[code](https://github.com/salesforce/LAVIS/tree/main/projects/img2llm-vqa)]
- (*arXiv 2023.06*) Retrieving-to-Answer: Zero-Shot Video Question Answering with Frozen Large Language Models [[paper](https://arxiv.org/pdf/2306.11732.pdf)]
- (*arXiv 2023.05*) ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions [[paper](https://arxiv.org/pdf/2303.06594.pdf)][[code](https://github.com/Vision-CAIR/ChatCaptioner)]
- (*arXiv 2023.02*) Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?[[paper](https://arxiv.org/pdf/2302.11713.pdf)]
- (*arXiv 2023.01*) See, Think, Confirm: Interactive Prompting Between Vision and Language Models for Knowledge-based Visual Reasoning [[paper](https://arxiv.org/pdf/2301.05226.pdf)]

## Contributing
üëç Contributions to this repository are welcome! 

If you have come across relevant resources, feel free to open an issue or submit a pull request.
```
- (*journal*) paper_name [[pdf]](link)[[code]](link)
```
